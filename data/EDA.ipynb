{"cells":[{"cell_type":"markdown","metadata":{"id":"n4twoscyrS6V"},"source":["# GameNet Project reproduciblity code"]},{"cell_type":"markdown","metadata":{"id":"5LdLt7hVK-6l"},"source":["# Introduction\n"]},{"cell_type":"markdown","metadata":{"id":"-hQrBND4LB8D"},"source":["Gamenet GAMENet: Graph Augmented MEmory networks for recommending medication combination by  tackles the limitation of instance-based recommendations with patient history to give better recommendations. It uses previous work that uses memory banks augmented by DDI graphs and dynamic memory based on patient history.  \n","\n","It uses a novel deep learning approach by constructing 2 RNN hidden layers, one for diagnosis and one for procedure. It uses this constructed patient representation with a pre-generated memory bank from DDI graph and EHR graph. The papers claim that this approach outperforms all current baselines in effectiveness measures, and achieved 3.60% DDI rate reduction. (Shang, J. et al (2019))\n"]},{"cell_type":"markdown","metadata":{"id":"55gydDcpLN3g"},"source":["# Scope of reproducibility\n"]},{"cell_type":"markdown","metadata":{"id":"FqSCVzhgLRky"},"source":["**Hypothesis 1**:\n","The papers claim that this approach outperforms all current baselines in effectiveness measures, and achieved 3.60% DDI rate reduction. We will reproduce the results with our model to verify this measure is achieved.\n","\n","\\\n","\n","\n","**Hypothesis 2**:\n","Author : Without DDI knowledge, GAMENet (w/o DDI) is also better than other methods which shows the overall framework does work\n","\n","We will evaluate if this holds true for the GAMENET model by comparing it with some pre established baselines created from other models.  \n"]},{"cell_type":"markdown","metadata":{"id":"j9XcQo4PL6mx"},"source":["# Data"]},{"cell_type":"markdown","metadata":{"id":"Pg-xZPH-LyJf"},"source":["●\tThe data  raw data is the diagnoses, prescriptions, procedure data from MIMIC-3 . A freely-available database comprising deidentified health-related data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012.( Johnson et al , 2016)\n","\n","●\tThe data also includes drug-drug interaction (TWOSIDEs dataset) .Drug side effects and drug-drug interactions were mined from publicly available dat. (Offside , n. d.)\n"]},{"cell_type":"markdown","metadata":{"id":"jJVKhScnMIuH"},"source":["#Refining the dataset\n","Note : the actual data processing generates pkl files and is being done at the bottom of the notebook . These files do not need to be regenerated and will be used in out model experiments below.\n","\n","## Data Split\n","Training data = 8/10\n","Validation data = 2/10\n","\n","##Create patient visit feature DB\n","●\twe will remove duplicate entries from the 3 datasets\\\n","●\tthe values will be sorted by subject_id\\\n","●\tthere will be one row for each visit in the input dataset to the model\\\n","●\twe will group all the diagnoses, medication, and procedure codes into arrays for every visit. The medication array would be one of the expected outputs\\\n","●\twe will convert NDC to ATC\\\n","●\twe might do some limits on the number of procedures, medications, and diagnoses to keep in our testing due to processing constraints\\\n","\n","# Creating feature vocabularies and patient record\n","Each diagnosis code, Medication NDC code, and procedure code will be mapped to a unique feature value. These feature vocabularies  will be used in our modeling\n","We will use the unique value to create another state of patient record which is numerical and therefore can be processed by the GAMENET model and any other model we may compare it against.\n","# Creating adjacency matrix\n","The adjacency matrix of DDI and NDC codes is needed to create the “memory banks” by running them through GCNs. These will be convoluted with input patient presentations to give an output. This is only needed to validate the results proposed in the model.\n"]},{"cell_type":"markdown","metadata":{"id":"I4hkPsyTrjgT"},"source":["# Steps to run and setup\n","STEP 1) Create a shortcut to the project in your home folder and run it from there\n","\n","STEP 2) **IMPORTANT** : Please replace the root folder of the project\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZX1XzqKHrNzB"},"outputs":[],"source":["path_to_root_folder = '/content/drive/MyDrive/DL/BDHC/GAMENet/'\n","# path_to_root_folder = \"/content/drive/MyDrive/GAMENet\"\n","path_to_code_folder=f\"{path_to_root_folder}/code\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29153,"status":"ok","timestamp":1711935725740,"user":{"displayName":"David Aderibigbe","userId":"01802288194141157255"},"user_tz":240},"id":"tPgbv0_v1Rcu","outputId":"25db5e78-5fa6-40b0-e62e-6574ff7e64bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running in Google Colab environment\n","Mounted at /content/drive/\n"]}],"source":["import os\n","\n","\n","def is_running_in_colab():\n","    return \"google.colab\" in str(get_ipython())\n","\n","if is_running_in_colab():\n","    print(\"Running in Google Colab environment\")\n","    is_colab = True\n","else:\n","    print(\"Not running in Google Colab environment\")\n","    is_colab = False\n","\n","if is_colab:\n","  from google.colab import drive\n","  drive.mount('/content/drive/')\n","  os.chdir(path_to_root_folder)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jfa0BRhh0_k-"},"outputs":[],"source":["import sys\n","sys.path.append(path_to_code_folder)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114343,"status":"ok","timestamp":1711936036665,"user":{"displayName":"David Aderibigbe","userId":"01802288194141157255"},"user_tz":240},"id":"k1BWe2P21MiD","outputId":"462a420a-b5dd-4f39-bdd1-dde3e81acf08"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/nvidia-cublas-cu12/\u001b[0m\u001b[33m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install dill -q\n","!pip install dnc -q"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711936788569,"user":{"displayName":"David Aderibigbe","userId":"01802288194141157255"},"user_tz":240},"id":"d5BVAxqdw93-","outputId":"90ad4607-761d-42e9-99ee-e798cb5e1e35"},"outputs":[{"output_type":"stream","name":"stdout","text":["Device available for running: \n","cpu\n"]}],"source":["import torch\n","\n","###############################\n","### DO NOT CHANGE THIS CELL ###\n","###############################\n","\n","# Use cuda if present\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device available for running: \")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"lL83olJb0ib7"},"source":["#Model Recreation Code."]},{"cell_type":"markdown","metadata":{"id":"WHEYiRsXM_0Q"},"source":["The cell below will define several helper function to help in training , validating , calculating scores and plotting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NwiIF7mu37yN"},"outputs":[],"source":["def train_with_ddi_awareness(model, data, optimizer, voc_size, device, lambda_ddi=0.1):\n","    \"\"\"\n","    Train the model with DDI awareness in the loss calculation.\n","\n","    :param model: The GAMENet model.\n","    :param data: The training data.\n","    :param optimizer: Optimizer.\n","    :param voc_size: Vocabulary size for the medications.\n","    :param device: Device to train on (CPU or GPU).\n","    :param lambda_ddi: Weight for the DDI-aware component of the loss.\n","    \"\"\"\n","    model.train()\n","    train_losses = []  # Lower the better\n","    ddi_scores = []  # Lower the better\n","\n","    for patient in data:\n","        for visits in patient:\n","            seq_input = torch.tensor(visits[:], dtype=torch.float).to(device)\n","            target = torch.zeros((1, voc_size[2]), dtype=torch.float).to(device)\n","            target[:, visits[2]] = 1\n","\n","            optimizer.zero_grad()\n","            output = model(seq_input)\n","            loss_bce = nn.functional.binary_cross_entropy_with_logits(output, target)\n","\n","            # Normalize output to calculate DDI score\n","            output_sigmoid = torch.sigmoid(output).detach()\n","            output_sigmoid[output_sigmoid >= 0.5] = 1\n","            output_sigmoid[output_sigmoid < 0.5] = 0\n","            y_label = np.where(output_sigmoid.cpu().numpy() == 1)[1]  # Adjusting axis for np.where\n","            ddi_score = ddi_rate_score([[y_label]])  # Assuming ddi_rate_score can handle this input format\n","\n","            # Incorporating DDI score into the loss\n","            loss = loss_bce + lambda_ddi * ddi_score\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_losses.append(loss.item())\n","            ddi_scores.append(ddi_score)\n","\n","    return train_losses, ddi_scores\n","\n","def validate_with_ddi_awareness(data, model, voc_size, device):\n","    \"\"\"\n","    Validate the model with DDI awareness.\n","\n","    :param data: The validation data.\n","    :param model: The GAMENet model.\n","    :param voc_size: Vocabulary size for the medications.\n","    :param device: Device to validate on (CPU or GPU).\n","    \"\"\"\n","    valid_losses = []  # Lower the better\n","    ddi_scores = []  # Lower the better\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for patient in data:\n","            for visits in patient:\n","                seq_input = torch.tensor(visits[:], dtype=torch.float).to(device)\n","                target = torch.zeros((1, voc_size[2]), dtype=torch.float).to(device)\n","                target[:, visits[2]] = 1\n","\n","                output = model(seq_input)\n","                loss_bce = nn.functional.binary_cross_entropy_with_logits(output, target)\n","\n","                # Normalize output to calculate DDI score\n","                output_sigmoid = torch.sigmoid(output).detach()\n","                output_sigmoid[output_sigmoid >= 0.5] = 1\n","                output_sigmoid[output_sigmoid < 0.5] = 0\n","                y_label = np.where(output_sigmoid.cpu().numpy() == 1)[1]  # Adjusting axis for np.where\n","                ddi_score = ddi_rate_score([[y_label]])  # Assuming ddi_rate_score can handle this input format\n","\n","                valid_losses.append(loss_bce.item())\n","                ddi_scores.append(ddi_score)\n","\n","    average_loss = np.mean(valid_losses)\n","    average_ddi_score = np.mean(ddi_scores)\n","\n","    return average_loss, average_ddi_score\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFNkG_Tt0f1w"},"outputs":[],"source":["import torch.nn as nn\n","import torch.optim as optim\n","import dill\n","# from models import GAMENet\n","import numpy as np\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","\n","#TODO : check if appropriate to copmare each position. The medication could be out of order. How to establish correct accuracy?\n","def accuracy_score(output, target):\n","    \"\"\"\n","    Calculate accuracy score of the medication predictions\n","    \"\"\"\n","    correct_predictions = np.sum(output == target)\n","    total_predictions = target.size\n","    return correct_predictions / total_predictions\n","\n","\n","def plot_learning_curves(train_losses, train_ddi_scores, valid_ddi_scores, train_accuracies=None, valid_losses=None, valid_accuracies=None):\n","  \"\"\"\n","  Plot the learnin curve for\n","    -training accuracies\n","    -training ddi scores\n","    -training losses\n","\n","    -valid accuracies\n","    -valid ddi scores\n","    -valid losses\n","\n","  \"\"\"\n","\t# TODO: Make plots for loss curves and accuracy curves.\n","\t# TODO: You do not have to return the plots.\n","\t# TODO: You can save plots as files by codes here or an interactive way according to your preference.\n","  width=10\n","  epochs = np.arange(0,len(train_losses))\n","\n","\t# Plot loss curves\n","\t# plt.figure(figsize=(10, 5))\n","  figure, axis  = plt.subplots(1, 3)\n","  if train_losses is not None :\n","      axis[0].plot(epochs, train_losses, label='Training Loss')\n","\n","  if valid_losses is not None :\n","      axis[0].plot(epochs, valid_losses, label='Validation Loss')\n","\n","  axis[0].set_xlabel('Epoch')\n","  axis[0].set_ylabel('Loss')\n","  axis[0].set_title('Loss Curves')\n","  axis[0].legend()\n","\n","  if train_accuracies is not None :\n","      axis[1].plot(epochs, train_accuracies, label='Training Accuracy')\n","  if valid_accuracies is not None :\n","      axis[1].plot(epochs, valid_accuracies, label='Validation Accuracy')\n","  axis[1].set_xlabel('Epoch')\n","  axis[1].set_ylabel('Accuracy')\n","  axis[1].set_title('Accuracy Curves')\n","  axis[1].legend()\n","\n","  #DDI SCORES\n","  if train_ddi_scores is not None :\n","      axis[2].plot(epochs, train_ddi_scores, label='Training DDI score')\n","      width = 15\n","  if valid_ddi_scores is not None :\n","      axis[2].plot(epochs, train_ddi_scores, label='Validation DDI Score')\n","      width = 15\n","\n","  axis[2].set_xlabel('Epoch')\n","  axis[2].set_ylabel('DDI_SCORE')\n","  axis[2].set_title('DDI_SCORE_GRAPH')\n","  axis[2].legend()\n","\n","\n","  figure.set_size_inches(w=width, h=5)\n","  plt.savefig('trainingplots.png')\n","  plt.show()\n","\n","def ddi_rate_score(record, path='data/ddi_A_final.pkl'):\n","    '''\n","    this calculates the ddi and we borrowed this from the original paper\n","    '''\n","    ddi_A = dill.load(open(path, 'rb'))\n","    all_cnt = 0\n","    dd_cnt = 0\n","    for patient in record:\n","        for adm in patient:\n","            med_code_set = adm\n","            for i, med_i in enumerate(med_code_set):\n","                for j, med_j in enumerate(med_code_set):\n","                    if j <= i:\n","                        continue\n","                    all_cnt += 1\n","                    if ddi_A[med_i, med_j] == 1 or ddi_A[med_j, med_i] == 1:\n","                        dd_cnt += 1\n","    if all_cnt == 0:\n","        return 0\n","    return dd_cnt / all_cnt\n","    # return all_cnt/dd_cnt\n","\n","def validate(data, model, voc_size):\n","\n","    valid_losses = []  #lower the better\n","    ddi_scores = [] #lower the better\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for patient, visits in enumerate(data):\n","            for idx, medical_features in enumerate(visits):\n","                seq_input = visits[:idx+1]  #get all the visits until the current visit\n","\n","                #target is the medication for the patient\n","                target = np.zeros((1, voc_size[2]))\n","                target[:, medical_features[2]] = 1\n","                # print(target)\n","                output = model(seq_input)\n","                loss = F.binary_cross_entropy_with_logits(output, torch.FloatTensor(target).to(device))\n","\n","                # normalising output\n","                output = F.sigmoid(output).detach().cpu().numpy()[0]\n","                output[output >= 0.5] = 1\n","                output[output < 0.5] = 0\n","                y_label = np.where(output == 1)[0]\n","                ddi_score = ddi_rate_score([[y_label]])\n","\n","                loss_i = float((loss.detach().cpu().numpy()))\n","                ddi_score_i = float(ddi_score)\n","\n","                # print(\"output is \",output)\n","                valid_accuracies = accuracy_score(output, target)\n","                valid_losses.append(loss_i)\n","                ddi_scores.append(ddi_score_i)\n","\n","    return valid_losses, valid_accuracies, ddi_scores\n","\n","def train(model,data,optimizer,voc_size):\n","    model.train()\n","    train_losses = []  #lower the better\n","    ddi_scores = [] #lower the better\n","\n","\n","    for patient, visits in enumerate(data):\n","        for idx, medical_features in enumerate(visits):\n","            seq_input = visits[:idx+1]  #get all the visits until the current visit\n","\n","            #target is the medication for the patient\n","            target = np.zeros((1, voc_size[2]))\n","            target[:, medical_features[2]] = 1\n","            # print(target)\n","            output, batch_neg_loss = model(seq_input)\n","            loss = F.binary_cross_entropy_with_logits(output, torch.FloatTensor(target).to(device))\n","\n","            # normalising output\n","            output = F.sigmoid(output).detach().cpu().numpy()[0]\n","            output[output >= 0.5] = 1\n","            output[output < 0.5] = 0\n","            y_label = np.where(output == 1)[0]\n","            ddi_score = ddi_rate_score([[y_label]])\n","\n","            loss_i = float((loss.detach().cpu().numpy()))\n","            ddi_score_i = float(ddi_score)\n","\n","            train_losses.append(loss_i)\n","            ddi_scores.append(ddi_score_i)\n","            optimizer.zero_grad()\n","            train_accuracies = accuracy_score(output, target)\n","            loss.backward(retain_graph=True)\n","            optimizer.step()\n","\n","    return train_losses, train_accuracies, ddi_scores\n","\n","def validate_cuda(data, model, voc_size, device = device):\n","    valid_losses = []  # Lower the better\n","    ddi_scores = []  # Lower the better\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for patient in data:\n","            for visits in patient:\n","                seq_input = torch.tensor(visits[:], dtype=torch.float).to(device)\n","                target = torch.zeros((1, voc_size[2]), dtype=torch.float).to(device)\n","                target[:, visits[2]] = 1  # Assuming this is the correct way to set the target\n","\n","                output = model(seq_input)\n","                loss = nn.functional.binary_cross_entropy_with_logits(output, target)\n","\n","                output = torch.sigmoid(output).cpu().numpy()[0]\n","                output[output >= 0.5] = 1\n","                output[output < 0.5] = 0\n","                y_label = np.where(output == 1)[0]\n","                ddi_score = ddi_rate_score([[y_label]])\n","\n","                valid_losses.append(loss.item())\n","                ddi_scores.append(float(ddi_score))\n","\n","    return valid_losses, ddi_scores\n","\n","def train_cuda(model, data, optimizer, voc_size, device = device):\n","    model.train()\n","    train_losses = []  # Lower the better\n","    ddi_scores = []  # Lower the better\n","\n","    for patient in data:\n","        for visits in patient:\n","            seq_input = torch.tensor(visits[:], dtype=torch.float).to(device)\n","            target = torch.zeros((1, voc_size[2]), dtype=torch.float).to(device)\n","            target[:, visits[2]] = 1  # Assuming this is the correct way to set the target\n","\n","            optimizer.zero_grad()\n","            output = model(seq_input)\n","            loss = nn.functional.binary_cross_entropy_with_logits(output, target)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            output = torch.sigmoid(output).detach().cpu().numpy()[0]\n","            output[output >= 0.5] = 1\n","            output[output < 0.5] = 0\n","            y_label = np.where(output == 1)[0]\n","            ddi_score = ddi_rate_score([[y_label]])\n","\n","            train_accuracies = accuracy_score(output, target)\n","\n","            train_losses.append(loss.item())\n","            ddi_scores.append(float(ddi_score))\n","\n","    return train_losses, train_accuracies, ddi_scores\n","\n"]},{"cell_type":"code","source":["import torch\n","import argparse\n","import numpy as np\n","import dill\n","import time\n","from torch.nn import CrossEntropyLoss\n","from torch.optim import Adam\n","import os\n","import torch.nn.functional as F\n","from collections import defaultdict\n","\n","from models import GAMENet\n","from util import llprint, multi_label_metric, ddi_rate_score, get_n_params\n","\n","torch.manual_seed(1203)\n","np.random.seed(1203)\n","\n","model_name = 'GAMENet'\n","resume_name = ''\n","\n","# Training settings\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--eval', action='store_true', default=False, help=\"eval mode\")\n","parser.add_argument('--model_name', type=str, default=model_name, help=\"model name\")\n","parser.add_argument('--resume_path', type=str, default=resume_name, help='resume path')\n","parser.add_argument('--ddi', action='store_true', default=False, help=\"using ddi\")\n","\n","args = parser.parse_args()\n","model_name = args.model_name\n","resume_name = args.resume_path\n","\n","def eval(model, data_eval, voc_size, epoch):\n","    # evaluate\n","    print('')\n","    model.eval()\n","    smm_record = []\n","    ja, prauc, avg_p, avg_r, avg_f1 = [[] for _ in range(5)]\n","    case_study = defaultdict(dict)\n","    med_cnt = 0\n","    visit_cnt = 0\n","    for step, input in enumerate(data_eval):\n","        y_gt = []\n","        y_pred = []\n","        y_pred_prob = []\n","        y_pred_label = []\n","        for adm_idx, adm in enumerate(input):\n","\n","            target_output1 = model(input[:adm_idx+1])\n","\n","            y_gt_tmp = np.zeros(voc_size[2])\n","            y_gt_tmp[adm[2]] = 1\n","            y_gt.append(y_gt_tmp)\n","\n","            target_output1 = F.sigmoid(target_output1).detach().cpu().numpy()[0]\n","            y_pred_prob.append(target_output1)\n","            y_pred_tmp = target_output1.copy()\n","            y_pred_tmp[y_pred_tmp>=0.5] = 1\n","            y_pred_tmp[y_pred_tmp<0.5] = 0\n","            y_pred.append(y_pred_tmp)\n","            y_pred_label_tmp = np.where(y_pred_tmp == 1)[0]\n","            y_pred_label.append(sorted(y_pred_label_tmp))\n","            visit_cnt += 1\n","            med_cnt += len(y_pred_label_tmp)\n","\n","\n","        smm_record.append(y_pred_label)\n","        adm_ja, adm_prauc, adm_avg_p, adm_avg_r, adm_avg_f1 = multi_label_metric(np.array(y_gt), np.array(y_pred), np.array(y_pred_prob))\n","        case_study[adm_ja] = {'ja': adm_ja, 'patient': input, 'y_label': y_pred_label}\n","\n","        ja.append(adm_ja)\n","        prauc.append(adm_prauc)\n","        avg_p.append(adm_avg_p)\n","        avg_r.append(adm_avg_r)\n","        avg_f1.append(adm_avg_f1)\n","        llprint('\\rEval--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_eval)))\n","\n","    # ddi rate\n","    ddi_rate = ddi_rate_score(smm_record)\n","\n","    llprint('\\tDDI Rate: %.4f, Jaccard: %.4f,  PRAUC: %.4f, AVG_PRC: %.4f, AVG_RECALL: %.4f, AVG_F1: %.4f\\n' % (\n","        ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n","    ))\n","    dill.dump(obj=smm_record, file=open('./data/gamenet_records.pkl', 'wb'))\n","    dill.dump(case_study, open(os.path.join('saved', model_name, 'case_study.pkl'), 'wb'))\n","\n","    # print('avg med', med_cnt / visit_cnt)\n","\n","    return ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n","\n","\n","def main():\n","    if not os.path.exists(os.path.join(\"saved\", model_name)):\n","        os.makedirs(os.path.join(\"saved\", model_name))\n","\n","    data_path = './data/records_final.pkl'\n","    voc_path = './data/voc_final.pkl'\n","\n","    ehr_adj_path = './data/ehr_adj_final.pkl'\n","    ddi_adj_path = './data/ddi_A_final.pkl'\n","    device = torch.device('cuda:0')\n","\n","    ehr_adj = dill.load(open(ehr_adj_path, 'rb'))\n","    ddi_adj = dill.load(open(ddi_adj_path, 'rb'))\n","    data = dill.load(open(data_path, 'rb'))\n","    voc = dill.load(open(voc_path, 'rb'))\n","    diag_voc, pro_voc, med_voc = voc['diag_voc'], voc['pro_voc'], voc['med_voc']\n","\n","    split_point = int(len(data) * 2 / 3)\n","    data_train = data[:split_point]\n","    eval_len = int(len(data[split_point:]) / 2)\n","    data_test = data[split_point:split_point + eval_len]\n","    data_eval = data[split_point+eval_len:]\n","\n","    EPOCH = 40\n","    LR = 0.0002\n","    TEST = args.eval\n","    Neg_Loss = args.ddi\n","    DDI_IN_MEM = args.ddi\n","    TARGET_DDI = 0.05\n","    T = 0.5\n","    decay_weight = 0.85\n","\n","    voc_size = (len(diag_voc.idx2word), len(pro_voc.idx2word), len(med_voc.idx2word))\n","    model = GAMENet(voc_size, ehr_adj, ddi_adj, emb_dim=64, device=device, ddi_in_memory=DDI_IN_MEM)\n","    if TEST:\n","        model.load_state_dict(torch.load(open(resume_name, 'rb')))\n","    model.to(device=device)\n","\n","    print('parameters', get_n_params(model))\n","    optimizer = Adam(list(model.parameters()), lr=LR)\n","\n","    if TEST:\n","        eval(model, data_test, voc_size, 0)\n","    else:\n","        history = defaultdict(list)\n","        best_epoch = 0\n","        best_ja = 0\n","        for epoch in range(EPOCH):\n","            loss_record1 = []\n","            start_time = time.time()\n","            model.train()\n","            prediction_loss_cnt = 0\n","            neg_loss_cnt = 0\n","            for step, input in enumerate(data_train):\n","                for idx, adm in enumerate(input):\n","                    seq_input = input[:idx+1]\n","                    loss1_target = np.zeros((1, voc_size[2]))\n","                    loss1_target[:, adm[2]] = 1\n","                    loss3_target = np.full((1, voc_size[2]), -1)\n","                    for idx, item in enumerate(adm[2]):\n","                        loss3_target[0][idx] = item\n","\n","                    target_output1, batch_neg_loss = model(seq_input)\n","\n","                    loss1 = F.binary_cross_entropy_with_logits(target_output1, torch.FloatTensor(loss1_target).to(device))\n","                    loss3 = F.multilabel_margin_loss(F.sigmoid(target_output1), torch.LongTensor(loss3_target).to(device))\n","                    if Neg_Loss:\n","                        target_output1 = F.sigmoid(target_output1).detach().cpu().numpy()[0]\n","                        target_output1[target_output1 >= 0.5] = 1\n","                        target_output1[target_output1 < 0.5] = 0\n","                        y_label = np.where(target_output1 == 1)[0]\n","                        current_ddi_rate = ddi_rate_score([[y_label]])\n","                        if current_ddi_rate <= TARGET_DDI:\n","                            loss = 0.9 * loss1 + 0.01 * loss3\n","                            prediction_loss_cnt += 1\n","                        else:\n","                            rnd = np.exp((TARGET_DDI - current_ddi_rate)/T)\n","                            if np.random.rand(1) < rnd:\n","                                loss = batch_neg_loss\n","                                neg_loss_cnt += 1\n","                            else:\n","                                loss = 0.9 * loss1 + 0.01 * loss3\n","                                prediction_loss_cnt += 1\n","                    else:\n","                        loss = 0.9 * loss1 + 0.01 * loss3\n","\n","                    optimizer.zero_grad()\n","                    loss.backward(retain_graph=True)\n","                    optimizer.step()\n","\n","                    loss_record1.append(loss.item())\n","\n","                llprint('\\rTrain--Epoch: %d, Step: %d/%d, L_p cnt: %d, L_neg cnt: %d' % (epoch, step, len(data_train), prediction_loss_cnt, neg_loss_cnt))\n","            # annealing\n","            T *= decay_weight\n","\n","            ddi_rate, ja, prauc, avg_p, avg_r, avg_f1 = eval(model, data_eval, voc_size, epoch)\n","\n","            history['ja'].append(ja)\n","            history['ddi_rate'].append(ddi_rate)\n","            history['avg_p'].append(avg_p)\n","            history['avg_r'].append(avg_r)\n","            history['avg_f1'].append(avg_f1)\n","            history['prauc'].append(prauc)\n","\n","            end_time = time.time()\n","            elapsed_time = (end_time - start_time) / 60\n","            llprint('\\tEpoch: %d, Loss: %.4f, One Epoch Time: %.2fm, Appro Left Time: %.2fh\\n' % (epoch,\n","                                                                                                np.mean(loss_record1),\n","                                                                                                elapsed_time,\n","                                                                                                elapsed_time * (\n","                                                                                                            EPOCH - epoch - 1)/60))\n","\n","            torch.save(model.state_dict(), open( os.path.join('saved', model_name, 'Epoch_%d_JA_%.4f_DDI_%.4f.model' % (epoch, ja, ddi_rate)), 'wb'))\n","            print('')\n","            if epoch != 0 and best_ja < ja:\n","                best_epoch = epoch\n","                best_ja = ja\n","\n","\n","        dill.dump(history, open(os.path.join('saved', model_name, 'history.pkl'), 'wb'))\n","\n","        # test\n","        torch.save(model.state_dict(), open(\n","            os.path.join('saved', model_name, 'final.model'), 'wb'))\n","\n","        print('best_epoch:', best_epoch)\n","\n","\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":661},"id":"YJxHUsgjU9ix","executionInfo":{"status":"error","timestamp":1711936839855,"user_tz":240,"elapsed":607,"user":{"displayName":"David Aderibigbe","userId":"01802288194141157255"}},"outputId":"9a90db3e-e7ad-4678-c88d-40eea6c285f8"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'jaccard_similarity_score' from 'sklearn.metrics' (/usr/local/lib/python3.10/dist-packages/sklearn/metrics/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-6b8ce4d7b044>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGAMENet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mllprint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_label_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mddi_rate_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_n_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1203\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/DL/BDHC/GAMENet//code/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaccard_similarity_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'jaccard_similarity_score' from 'sklearn.metrics' (/usr/local/lib/python3.10/dist-packages/sklearn/metrics/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","metadata":{"id":"5WBkNO3oN-1K"},"source":["## The cells below is doing a short description of our model and defining it in code."]},{"cell_type":"markdown","metadata":{"id":"5NP7-FNNOjaB"},"source":["# Modelling Write up"]},{"cell_type":"markdown","metadata":{"id":"tSHQ2jJrOm2l"},"source":["Our final model will follow the architecture outlined in the paper.\n","\n","### Data Input\n","There is 2 visit sequences that are ran through the model the include history of procedures and diagnoses by another patient .  They are of size , (visit_dim, procedure_count) and (visit_dim,diagnoses_count). Visit size will include the 0th visit to the current visit. We will also send the in the medication in the same format as it will be used for key value store.\n","For initial experiment we are using\\\n","Diagnosis   count \t1958\n","Medications prescribed count\t145\n","Procedure count\t1426\\\n","\n","###Embedding Network\n","Each vector(medication, diagnosis, procedure) will be reduced using an embedding which will output a size emb_dim which is set at 64 for now.\n","Dual RNN\\\n","GRU will be used for this step . We will have one GRU for diagnoses and one for procedures. The input dimension would be 64 as that is the output of embedding network. The output dimension will be doubled.\n","###Patient Representation and query\n","We combine the GRU results by doing a concatenation to create a patient representation of procedure and diagnoses.\n","We run this through a linear layer with RelU to get a queries for the entire sequence.\n","The current query is the last element .\n","### Memory Bank\n","This is created using the adjacency matrix created in the data pre-processing sections. We created 2 Graph convoluted networks, one for EHR adjacency matrix and one for DDI. We use the implementation provided in the paper to create these GCNs. It doesn’t have input, just weights associated with the adjacency matrix that are adjusted during training\n","###Key Value store\n","The keys are all the queries for the past visits. This is taken from the GRU results.\n","The values are the medication for the past visits.\n","### Ob and od\n","Ob = memory_bank_contribution = softmax(query  . memory_bank_transpose). Memory_bank \\\n","Od = key-store contribution = memory_bank . (key_value_store_values). Softmax(key store keys . query)\n","\n","### Sigmoid Output\n","We apply a sigmoid out to each of the outputs to scale them between 0 and 1. Anything above 0.5 is assumed 1 and anything below 0.5 is assumed 0.\n","###Combined Loss\n","The target is the medication array for the visit for which we calculate the loss score. For initial iteration we are using binary cross entropy with logits as described in the paper. For final results we will be using will take different transformation of the output which will be combined to give a final result.\n","###DDI Score\n","The features of medications sigmoid output will be fed into the ddi_score function to calculate drug-drug interaction scores. We will aim to minimize this function along with the loss of the predicted medications.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bjmaZ6W8N-AG"},"outputs":[],"source":["from models import GCN\n","\n","class GAMENet(nn.Module):\n","    def __init__(self, vocab_size, ehr_adj, ddi_adj, emb_dim=64, device=torch.device('cpu:0'), ddi_in_memory=True):\n","        super(GAMENet, self).__init__()\n","        K = len(vocab_size)\n","        self.K = K\n","        self.vocab_size = vocab_size\n","        self.device = device\n","        self.tensor_ddi_adj = torch.FloatTensor(ddi_adj).to(device)\n","        self.ddi_in_memory = ddi_in_memory\n","        self.embeddings = nn.ModuleList(\n","            [nn.Embedding(vocab_size[i], emb_dim) for i in range(K-1)])\n","        self.dropout = nn.Dropout(p=0.4)\n","\n","        self.encoders = nn.ModuleList([nn.GRU(emb_dim, emb_dim*2, batch_first=True) for _ in range(K-1)])\n","\n","        self.query = nn.Sequential(\n","            nn.ReLU(),\n","            nn.Linear(emb_dim * 4, emb_dim),\n","        )\n","\n","        self.ehr_gcn = GCN(voc_size=vocab_size[2], emb_dim=emb_dim, adj=ehr_adj, device=device)\n","        self.ddi_gcn = GCN(voc_size=vocab_size[2], emb_dim=emb_dim, adj=ddi_adj, device=device)\n","        self.inter = nn.Parameter(torch.FloatTensor(1))\n","\n","        self.output = nn.Sequential(\n","            nn.ReLU(),\n","            nn.Linear(emb_dim * 3, emb_dim * 2),\n","            nn.ReLU(),\n","            nn.Linear(emb_dim * 2, vocab_size[2])\n","        )\n","\n","        self.init_weights()\n","\n","    def forward(self, input):\n","        # input (adm, 3, codes)\n","\n","        # generate medical embeddings and queries\n","        i1_seq = []\n","        i2_seq = []\n","        def mean_embedding(embedding):\n","            return embedding.mean(dim=1).unsqueeze(dim=0)  # (1,1,dim)\n","        for adm in input:\n","            i1 = mean_embedding(self.dropout(self.embeddings[0](torch.LongTensor(adm[0]).unsqueeze(dim=0).to(self.device)))) # (1,1,dim)\n","            i2 = mean_embedding(self.dropout(self.embeddings[1](torch.LongTensor(adm[1]).unsqueeze(dim=0).to(self.device))))\n","            i1_seq.append(i1)\n","            i2_seq.append(i2)\n","        i1_seq = torch.cat(i1_seq, dim=1) #(1,seq,dim)\n","        i2_seq = torch.cat(i2_seq, dim=1) #(1,seq,dim)\n","\n","        o1, h1 = self.encoders[0](\n","            i1_seq\n","        ) # o1:(1, seq, dim*2) hi:(1,1,dim*2)\n","        o2, h2 = self.encoders[1](\n","            i2_seq\n","        )\n","        patient_representations = torch.cat([o1, o2], dim=-1).squeeze(dim=0) # (seq, dim*4)\n","        queries = self.query(patient_representations) # (seq, dim)\n","\n","        # graph memory module\n","        '''I:generate current input'''\n","        query = queries[-1:] # (1,dim)\n","\n","        '''G:generate graph memory bank and insert history information'''\n","        if self.ddi_in_memory:\n","            drug_memory = self.ehr_gcn() - self.ddi_gcn() * self.inter  # (size, dim)\n","        else:\n","            drug_memory = self.ehr_gcn()\n","\n","        if len(input) > 1:\n","            history_keys = queries[:(queries.size(0)-1)] # (seq-1, dim)\n","\n","            history_values = np.zeros((len(input)-1, self.vocab_size[2]))\n","            for idx, adm in enumerate(input):\n","                if idx == len(input)-1:\n","                    break\n","                history_values[idx, adm[2]] = 1\n","            history_values = torch.FloatTensor(history_values).to(self.device) # (seq-1, size)\n","\n","        '''O:read from global memory bank and dynamic memory bank'''\n","        key_weights1 = F.softmax(torch.mm(query, drug_memory.t()), dim=-1)  # (1, size)\n","        fact1 = torch.mm(key_weights1, drug_memory)  # (1, dim)\n","\n","        if len(input) > 1:\n","            visit_weight = F.softmax(torch.mm(query, history_keys.t())) # (1, seq-1)\n","            weighted_values = visit_weight.mm(history_values) # (1, size)\n","            fact2 = torch.mm(weighted_values, drug_memory) # (1, dim)\n","        else:\n","            fact2 = fact1\n","        '''R:convert O and predict'''\n","        output = self.output(torch.cat([query, fact1, fact2], dim=-1)) # (1, dim)\n","\n","        if self.training:\n","            neg_pred_prob = F.sigmoid(output)\n","            neg_pred_prob = neg_pred_prob.t() * neg_pred_prob  # (voc_size, voc_size)\n","            batch_neg = neg_pred_prob.mul(self.tensor_ddi_adj).mean()\n","\n","            return output, batch_neg\n","        else:\n","            return output\n","\n","    def init_weights(self):\n","        \"\"\"Initialize weights.\"\"\"\n","        initrange = 0.1\n","        for item in self.embeddings:\n","            item.weight.data.uniform_(-initrange, initrange)\n","\n","        self.inter.data.uniform_(-initrange, initrange)"]},{"cell_type":"markdown","metadata":{"id":"eyT3mB6DOKOI"},"source":["##The cell below is running the training and validation to generate the plots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTPXG9HDMzvJ"},"outputs":[],"source":["# Some parameters\n","NUM_EPOCHS = 25\n","USE_CUDA = True  # Set 'True' if you want to use GPU\n","NUM_WORKERS = 4  # Number of threads used by DataLoader. You can adjust this according to your machine spec.\n","\n","device = torch.device(\"cuda\" if USE_CUDA and torch.cuda.is_available() else \"cpu\")\n","torch.manual_seed(1)\n","\n","# LOADING DATA\n","data_path = 'data/records_final.pkl'\n","voc_path = 'data/voc_final.pkl'\n","voc_path = 'voc_final.pkl'\n","\n","ehr_adj_path = 'data/ehr_adj_final.pkl'\n","ddi_adj_path = 'data/ddi_A_final.pkl'\n","# device = torch.device('cpu')\n","\n","ehr_adj = dill.load(open(ehr_adj_path, 'rb'))\n","ddi_adj = dill.load(open(ddi_adj_path, 'rb'))\n","data = dill.load(open(data_path, 'rb'))\n","voc = dill.load(open(voc_path, 'rb'))\n","diag_voc, pro_voc, med_voc = voc['diag_voc'], voc['pro_voc'], voc['med_voc']\n","voc_size = (len(diag_voc.idx2word), len(pro_voc.idx2word), len(med_voc.idx2word))\n","\n","# MODEL\n","print(f\"running model in {device}\")\n","model = GAMENet(voc_size, ehr_adj, ddi_adj, emb_dim=64, device=device, ddi_in_memory=True)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters())\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print (f\"Using device {device}\")\n","model.to(device)\n","criterion.to(device)\n","\n","\n","# DATA SPLIT\n","split_point = int(len(data) * 8/10)\n","data_train = data[:split_point]\n","# data_valid\n","\n","# TRAINING\n","#medical_features[0] = diagnoses\n","#medical_features[1] = procedures\n","#medical_features[2] = medication\n","\n","# INIT TRACKERS\n","average_train_losses = []  #lower the better\n","average_train_accuracies = []\n","average_train_ddi_scores = [] #lower the better\n","average_valid_losses  = []\n","average_valid_ddi_scores = []\n","average_valid_accuracies = []\n","\n","for epoch in range(NUM_EPOCHS):\n","    print(f\"Running epoch {epoch}\")\n","    train_losses, train_accuracies, train_ddi_scores = train(model=model, data=data_train,optimizer=optimizer,voc_size=voc_size)\n","    valid_losses, valid_accuracies, valid_ddi_scores = validate(model=model,data=data_train,voc_size=voc_size)\n","\n","    average_train_losses.append(np.average(np.array(train_losses)))\n","    average_train_accuracies.append(np.average(train_accuracies))\n","    average_train_ddi_scores.append(np.average(np.array(train_ddi_scores)))\n","\n","\n","    average_valid_losses.append(np.average(np.array(valid_losses)))\n","    average_valid_accuracies.append(np.average(valid_accuracies))\n","    average_valid_ddi_scores.append(np.average(np.array(valid_ddi_scores)))\n","\n","    plot_learning_curves(\n","        train_losses=average_train_losses,\n","        valid_losses=average_valid_losses,\n","        train_ddi_scores=average_train_ddi_scores,\n","        valid_ddi_scores=average_valid_ddi_scores\n","        )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qi4GXPEoSGfa"},"outputs":[],"source":["average_train_accuracies"]},{"cell_type":"markdown","metadata":{"id":"tMXrYCpcbB-Y"},"source":["# Data Processing - You can run this only if you want to regenerate the data files but this is not necessary\n"]},{"cell_type":"markdown","metadata":{"id":"ebScjukTyO0m"},"source":["## Read from MIMIC csv files"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"1vzSmox5yO0o","outputId":"9b6a8c89-6c3c-4d86-fa22-40297eb1ec79"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-29-0d55a0750f53>:41: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n","  med_pd = pd.read_csv(med_file, dtype={'NDC':'category'})\n"]},{"name":"stdout","output_type":"stream","text":["#patients  (6350,)\n","#clinical events  15016\n","#diagnosis  1958\n","#med  145\n","#procedure 1426\n","#avg of diagnoses  10.514717634523175\n","#avg of medicines  8.80420884389984\n","#avg of procedures  3.8445657964837507\n","#avg of vists  2.3647244094488187\n","#max of diagnoses  128\n","#max of medicines  55\n","#max of procedures  50\n","#max of visit  29\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"data\",\n  \"rows\": 15016,\n  \"fields\": [\n    {\n      \"column\": \"SUBJECT_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 27835,\n        \"min\": 17,\n        \"max\": 99982,\n        \"num_unique_values\": 6350,\n        \"samples\": [\n          5937,\n          17122,\n          25049\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HADM_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28855,\n        \"min\": 100003,\n        \"max\": 199995,\n        \"num_unique_values\": 15016,\n        \"samples\": [\n          176772,\n          132788,\n          183430\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ICD9_CODE\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NDC\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PRO_CODE\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NDC_Len\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 1,\n        \"max\": 36,\n        \"num_unique_values\": 36,\n        \"samples\": [\n          34,\n          8,\n          26\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"data"},"text/html":["\n","  <div id=\"df-fa484151-2c82-4382-9903-d60f73605351\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>SUBJECT_ID</th>\n","      <th>HADM_ID</th>\n","      <th>ICD9_CODE</th>\n","      <th>NDC</th>\n","      <th>PRO_CODE</th>\n","      <th>NDC_Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>17</td>\n","      <td>161087</td>\n","      <td>[4239, 5119, 78551, 4589, 311, 7220, 71946, 2724]</td>\n","      <td>[N02B, A01A, A02B, A06A, B05C, A12A, A12C, C01...</td>\n","      <td>[3731, 8872, 3893]</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>17</td>\n","      <td>194023</td>\n","      <td>[7455, 45829, V1259, 2724]</td>\n","      <td>[N02B, A01A, A02B, A06A, A12A, B05C, A12C, C01...</td>\n","      <td>[3571, 3961, 8872]</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>21</td>\n","      <td>109451</td>\n","      <td>[41071, 78551, 5781, 5849, 40391, 4280, 4592, ...</td>\n","      <td>[A06A, C07A, A12A, A02A, J01M, C02A, B05C, B01...</td>\n","      <td>[0066, 3761, 3950, 3606, 0042, 0047, 3895, 399...</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>21</td>\n","      <td>111970</td>\n","      <td>[0388, 78552, 40391, 42731, 70709, 5119, 6823,...</td>\n","      <td>[A06A, B05C, A12C, A07A, N02B, B01A, N06A, A01...</td>\n","      <td>[3995, 8961, 0014]</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>23</td>\n","      <td>124321</td>\n","      <td>[2252, 3485, 78039, 4241, 4019, 2720, 2724, V4...</td>\n","      <td>[C07A, N02B, A02B, H03A, N03A, A01A, N05A, C09...</td>\n","      <td>[0151]</td>\n","      <td>11</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa484151-2c82-4382-9903-d60f73605351')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-fa484151-2c82-4382-9903-d60f73605351 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-fa484151-2c82-4382-9903-d60f73605351');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-d63cd23b-71e9-438f-bc10-c39d8930fb92\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d63cd23b-71e9-438f-bc10-c39d8930fb92')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-d63cd23b-71e9-438f-bc10-c39d8930fb92 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["   SUBJECT_ID  HADM_ID                                          ICD9_CODE  \\\n","0          17   161087  [4239, 5119, 78551, 4589, 311, 7220, 71946, 2724]   \n","1          17   194023                         [7455, 45829, V1259, 2724]   \n","2          21   109451  [41071, 78551, 5781, 5849, 40391, 4280, 4592, ...   \n","3          21   111970  [0388, 78552, 40391, 42731, 70709, 5119, 6823,...   \n","4          23   124321  [2252, 3485, 78039, 4241, 4019, 2720, 2724, V4...   \n","\n","                                                 NDC  \\\n","0  [N02B, A01A, A02B, A06A, B05C, A12A, A12C, C01...   \n","1  [N02B, A01A, A02B, A06A, A12A, B05C, A12C, C01...   \n","2  [A06A, C07A, A12A, A02A, J01M, C02A, B05C, B01...   \n","3  [A06A, B05C, A12C, A07A, N02B, B01A, N06A, A01...   \n","4  [C07A, N02B, A02B, H03A, N03A, A01A, N05A, C09...   \n","\n","                                            PRO_CODE  NDC_Len  \n","0                                 [3731, 8872, 3893]       14  \n","1                                 [3571, 3961, 8872]       15  \n","2  [0066, 3761, 3950, 3606, 0042, 0047, 3895, 399...       17  \n","3                                 [3995, 8961, 0014]       17  \n","4                                             [0151]       11  "]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import os\n","# files can be downloaded from https://mimic.physionet.org/gettingstarted/dbsetup/\n","if is_colab:\n","    local_path = f\"{path_to_root_folder}/data/\"\n","    os.chdir(local_path)\n","else:\n","    local_path = ''\n","\n","med_file = 'PRESCRIPTIONS.csv'\n","diag_file =  'DIAGNOSES_ICD.csv'\n","procedure_file =  'PROCEDURES_ICD.csv'\n","\n","# drug code mapping files (already in ./data/)\n","ndc2atc_file = 'ndc2atc_level4.csv'\n","cid_atc = 'drug-atc.csv'\n","ndc_rxnorm_file = 'ndc2rxnorm_mapping.txt'\n","\n","# drug-drug interactions can be down https://www.dropbox.com/s/8os4pd2zmp2jemd/drug-DDI.csv?dl=0\n","ddi_file = 'drug-DDI.csv'\n","\n","def process_procedure():\n","    pro_pd = pd.read_csv(procedure_file, dtype={'ICD9_CODE':'category'})\n","    pro_pd.drop(columns=['ROW_ID'], inplace=True)\n","#     pro_pd = pro_pd[pro_pd['SEQ_NUM']<5]\n","#     def icd9_tree(x):\n","#         if x[0]=='E':\n","#             return x[:4]\n","#         return x[:3]\n","#     pro_pd['ICD9_CODE'] = pro_pd['ICD9_CODE'].map(icd9_tree)\n","    pro_pd.drop_duplicates(inplace=True)\n","    pro_pd.sort_values(by=['SUBJECT_ID', 'HADM_ID', 'SEQ_NUM'], inplace=True)\n","    pro_pd.drop(columns=['SEQ_NUM'], inplace=True)\n","    pro_pd.drop_duplicates(inplace=True)\n","    pro_pd.reset_index(drop=True, inplace=True)\n","\n","    return pro_pd\n","\n","\n","def process_med():\n","    med_pd = pd.read_csv(med_file, dtype={'NDC':'category'})\n","    # filter\n","    med_pd.drop(columns=['ROW_ID','DRUG_TYPE','DRUG_NAME_POE','DRUG_NAME_GENERIC',\n","                     'FORMULARY_DRUG_CD','GSN','PROD_STRENGTH','DOSE_VAL_RX',\n","                     'DOSE_UNIT_RX','FORM_VAL_DISP','FORM_UNIT_DISP','FORM_UNIT_DISP',\n","                      'ROUTE','ENDDATE','DRUG'], axis=1, inplace=True)\n","    med_pd.drop(index = med_pd[med_pd['NDC'] == '0'].index, axis=0, inplace=True)\n","    med_pd.fillna(method='pad', inplace=True)\n","    med_pd.dropna(inplace=True)\n","    med_pd.drop_duplicates(inplace=True)\n","    med_pd['ICUSTAY_ID'] = med_pd['ICUSTAY_ID'].astype('int64')\n","    med_pd['STARTDATE'] = pd.to_datetime(med_pd['STARTDATE'], format='%Y-%m-%d %H:%M:%S')\n","    med_pd.sort_values(by=['SUBJECT_ID', 'HADM_ID', 'ICUSTAY_ID', 'STARTDATE'], inplace=True)\n","    med_pd = med_pd.reset_index(drop=True)\n","\n","    def filter_first24hour_med(med_pd):\n","        med_pd_new = med_pd.drop(columns=['NDC'])\n","        med_pd_new = med_pd_new.groupby(by=['SUBJECT_ID','HADM_ID','ICUSTAY_ID']).head(1).reset_index(drop=True)\n","        med_pd_new = pd.merge(med_pd_new, med_pd, on=['SUBJECT_ID','HADM_ID','ICUSTAY_ID','STARTDATE'])\n","        med_pd_new = med_pd_new.drop(columns=['STARTDATE'])\n","        return med_pd_new\n","    med_pd = filter_first24hour_med(med_pd)\n","#     med_pd = med_pd.drop(columns=['STARTDATE'])\n","\n","    med_pd = med_pd.drop(columns=['ICUSTAY_ID'])\n","    med_pd = med_pd.drop_duplicates()\n","    med_pd = med_pd.reset_index(drop=True)\n","\n","    # visit > 2\n","    def process_visit_lg2(med_pd):\n","        a = med_pd[['SUBJECT_ID', 'HADM_ID']].groupby(by='SUBJECT_ID')['HADM_ID'].unique().reset_index()\n","        a['HADM_ID_Len'] = a['HADM_ID'].map(lambda x:len(x))\n","        a = a[a['HADM_ID_Len'] > 1]\n","        return a\n","    med_pd_lg2 = process_visit_lg2(med_pd).reset_index(drop=True)\n","    med_pd = med_pd.merge(med_pd_lg2[['SUBJECT_ID']], on='SUBJECT_ID', how='inner')\n","\n","    return med_pd.reset_index(drop=True)\n","\n","def process_diag():\n","    diag_pd = pd.read_csv(diag_file)\n","    diag_pd.dropna(inplace=True)\n","#     def icd9_tree(x):\n","#         if x[0]=='E':\n","#             return x[:4]\n","#         return x[:3]\n","#     diag_pd['ICD9_CODE'] = diag_pd['ICD9_CODE'].map(icd9_tree)\n","#     diag_pd = diag_pd[diag_pd['SEQ_NUM'] < 5]\n","    diag_pd.drop(columns=['SEQ_NUM','ROW_ID'],inplace=True)\n","    diag_pd.drop_duplicates(inplace=True)\n","    diag_pd.sort_values(by=['SUBJECT_ID','HADM_ID'], inplace=True)\n","    return diag_pd.reset_index(drop=True)\n","\n","def ndc2atc4(med_pd):\n","    with open(ndc_rxnorm_file, 'r') as f:\n","        ndc2rxnorm = eval(f.read())\n","    med_pd['RXCUI'] = med_pd['NDC'].map(ndc2rxnorm)\n","    med_pd.dropna(inplace=True)\n","\n","    rxnorm2atc = pd.read_csv(ndc2atc_file)\n","    rxnorm2atc = rxnorm2atc.drop(columns=['YEAR','MONTH','NDC'])\n","    rxnorm2atc.drop_duplicates(subset=['RXCUI'], inplace=True)\n","    med_pd.drop(index = med_pd[med_pd['RXCUI'].isin([''])].index, axis=0, inplace=True)\n","\n","    med_pd['RXCUI'] = med_pd['RXCUI'].astype('int64')\n","    med_pd = med_pd.reset_index(drop=True)\n","    med_pd = med_pd.merge(rxnorm2atc, on=['RXCUI'])\n","    med_pd.drop(columns=['NDC', 'RXCUI'], inplace=True)\n","    med_pd = med_pd.rename(columns={'ATC4':'NDC'})\n","    med_pd['NDC'] = med_pd['NDC'].map(lambda x: x[:4])\n","    med_pd = med_pd.drop_duplicates()\n","    med_pd = med_pd.reset_index(drop=True)\n","    return med_pd\n","\n","def filter_1000_most_pro(pro_pd):\n","    pro_count = pro_pd.groupby(by=['ICD9_CODE']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n","    pro_pd = pro_pd[pro_pd['ICD9_CODE'].isin(pro_count.loc[:1000, 'ICD9_CODE'])]\n","\n","    return pro_pd.reset_index(drop=True)\n","\n","def filter_2000_most_diag(diag_pd):\n","    diag_count = diag_pd.groupby(by=['ICD9_CODE']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n","    diag_pd = diag_pd[diag_pd['ICD9_CODE'].isin(diag_count.loc[:1999, 'ICD9_CODE'])]\n","\n","    return diag_pd.reset_index(drop=True)\n","\n","def filter_300_most_med(med_pd):\n","    med_count = med_pd.groupby(by=['NDC']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n","    med_pd = med_pd[med_pd['NDC'].isin(med_count.loc[:299, 'NDC'])]\n","\n","    return med_pd.reset_index(drop=True)\n","\n","def process_all():\n","    # get med and diag (visit>=2)\n","    med_pd = process_med()\n","    med_pd = ndc2atc4(med_pd)\n","#     med_pd = filter_300_most_med(med_pd)\n","\n","    diag_pd = process_diag()\n","    diag_pd = filter_2000_most_diag(diag_pd)\n","\n","    pro_pd = process_procedure()\n","#     pro_pd = filter_1000_most_pro(pro_pd)\n","\n","    med_pd_key = med_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n","    diag_pd_key = diag_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n","    pro_pd_key = pro_pd[['SUBJECT_ID', 'HADM_ID']].drop_duplicates()\n","\n","    combined_key = med_pd_key.merge(diag_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n","    combined_key = combined_key.merge(pro_pd_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n","\n","    diag_pd = diag_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n","    med_pd = med_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n","    pro_pd = pro_pd.merge(combined_key, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n","\n","    # flatten and merge\n","    diag_pd = diag_pd.groupby(by=['SUBJECT_ID','HADM_ID'])['ICD9_CODE'].unique().reset_index()\n","    med_pd = med_pd.groupby(by=['SUBJECT_ID', 'HADM_ID'])['NDC'].unique().reset_index()\n","    pro_pd = pro_pd.groupby(by=['SUBJECT_ID','HADM_ID'])['ICD9_CODE'].unique().reset_index().rename(columns={'ICD9_CODE':'PRO_CODE'})\n","    med_pd['NDC'] = med_pd['NDC'].map(lambda x: list(x))\n","    pro_pd['PRO_CODE'] = pro_pd['PRO_CODE'].map(lambda x: list(x))\n","    data = diag_pd.merge(med_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n","    data = data.merge(pro_pd, on=['SUBJECT_ID', 'HADM_ID'], how='inner')\n","#     data['ICD9_CODE_Len'] = data['ICD9_CODE'].map(lambda x: len(x))\n","    data['NDC_Len'] = data['NDC'].map(lambda x: len(x))\n","    return data\n","\n","def statistics():\n","    print('#patients ', data['SUBJECT_ID'].unique().shape)\n","    print('#clinical events ', len(data))\n","\n","    diag = data['ICD9_CODE'].values\n","    med = data['NDC'].values\n","    pro = data['PRO_CODE'].values\n","\n","    unique_diag = set([j for i in diag for j in list(i)])\n","    unique_med = set([j for i in med for j in list(i)])\n","    unique_pro = set([j for i in pro for j in list(i)])\n","\n","    print('#diagnosis ', len(unique_diag))\n","    print('#med ', len(unique_med))\n","    print('#procedure', len(unique_pro))\n","\n","    avg_diag = 0\n","    avg_med = 0\n","    avg_pro = 0\n","    max_diag = 0\n","    max_med = 0\n","    max_pro = 0\n","    cnt = 0\n","    max_visit = 0\n","    avg_visit = 0\n","\n","    for subject_id in data['SUBJECT_ID'].unique():\n","        item_data = data[data['SUBJECT_ID'] == subject_id]\n","        x = []\n","        y = []\n","        z = []\n","        visit_cnt = 0\n","        for index, row in item_data.iterrows():\n","            visit_cnt += 1\n","            cnt += 1\n","            x.extend(list(row['ICD9_CODE']))\n","            y.extend(list(row['NDC']))\n","            z.extend(list(row['PRO_CODE']))\n","        x = set(x)\n","        y = set(y)\n","        z = set(z)\n","        avg_diag += len(x)\n","        avg_med += len(y)\n","        avg_pro += len(z)\n","        avg_visit += visit_cnt\n","        if len(x) > max_diag:\n","            max_diag = len(x)\n","        if len(y) > max_med:\n","            max_med = len(y)\n","        if len(z) > max_pro:\n","            max_pro = len(z)\n","        if visit_cnt > max_visit:\n","            max_visit = visit_cnt\n","\n","\n","\n","    print('#avg of diagnoses ', avg_diag/ cnt)\n","    print('#avg of medicines ', avg_med/ cnt)\n","    print('#avg of procedures ', avg_pro/ cnt)\n","    print('#avg of vists ', avg_visit/ len(data['SUBJECT_ID'].unique()))\n","\n","\n","    print('#max of diagnoses ', max_diag)\n","    print('#max of medicines ', max_med)\n","    print('#max of procedures ', max_pro)\n","    print('#max of visit ', max_visit)\n","\n","\n","\n","data = process_all()\n","statistics()\n","data.to_pickle('data_final.pkl')\n","data.head()"]},{"cell_type":"markdown","metadata":{"id":"7VqW2qdJyO0s"},"source":["## Create Vocaboray for Medical Codes & Save Patient Record in pickle form"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8847,"status":"ok","timestamp":1711870749507,"user":{"displayName":"Taha Hussain","userId":"03136328795343180859"},"user_tz":360},"id":"b5CqiCRoyO0t","outputId":"d9fd929c-7e54-4cab-99bf-1f9c397a9b87"},"outputs":[{"data":{"text/plain":["(1958, 145, 1426)"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["import dill\n","import pandas as pd\n","class Voc(object):\n","    def __init__(self):\n","        self.idx2word = {}\n","        self.word2idx = {}\n","\n","    def add_sentence(self, sentence):\n","        for word in sentence:\n","            if word not in self.word2idx:\n","                self.idx2word[len(self.word2idx)] = word\n","                self.word2idx[word] = len(self.word2idx)\n","\n","def create_str_token_mapping(df):\n","    diag_voc = Voc()\n","    med_voc = Voc()\n","    pro_voc = Voc()\n","    ## only for DMNC\n","#     diag_voc.add_sentence(['seperator', 'decoder_point'])\n","#     med_voc.add_sentence(['seperator', 'decoder_point'])\n","#     pro_voc.add_sentence(['seperator', 'decoder_point'])\n","\n","    for index, row in df.iterrows():\n","        diag_voc.add_sentence(row['ICD9_CODE'])\n","        med_voc.add_sentence(row['NDC'])\n","        pro_voc.add_sentence(row['PRO_CODE'])\n","\n","    dill.dump(obj={'diag_voc':diag_voc, 'med_voc':med_voc ,'pro_voc':pro_voc}, file=open('voc_final.pkl','wb'))\n","    return diag_voc, med_voc, pro_voc\n","\n","def create_patient_record(df, diag_voc, med_voc, pro_voc):\n","    records = [] # (patient, code_kind:3, codes)  code_kind:diag, proc, med\n","    for subject_id in df['SUBJECT_ID'].unique():\n","        item_df = df[df['SUBJECT_ID'] == subject_id]\n","        patient = []\n","        for index, row in item_df.iterrows():\n","            admission = []\n","            admission.append([diag_voc.word2idx[i] for i in row['ICD9_CODE']])\n","            admission.append([pro_voc.word2idx[i] for i in row['PRO_CODE']])\n","            admission.append([med_voc.word2idx[i] for i in row['NDC']])\n","            patient.append(admission)\n","        records.append(patient)\n","    dill.dump(obj=records, file=open('records_final.pkl', 'wb'))\n","    return records\n","\n","\n","path='data_final.pkl'\n","df = pd.read_pickle(path)\n","diag_voc, med_voc, pro_voc = create_str_token_mapping(df)\n","records = create_patient_record(df, diag_voc, med_voc, pro_voc)\n","len(diag_voc.idx2word), len(med_voc.idx2word), len(pro_voc.idx2word)"]},{"cell_type":"markdown","metadata":{"id":"659e6-B5yO0u"},"source":["## DDI & Construct EHR Adj and DDI Adj"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":24,"status":"error","timestamp":1711870749507,"user":{"displayName":"Taha Hussain","userId":"03136328795343180859"},"user_tz":360},"id":"_q4NDjBzyO0u","outputId":"145e0fdf-31d2-43f4-9bb6-93d175fc21e2","scrolled":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'drug-atc.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-8c691d64d0e7>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcid_atc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mline_ls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drug-atc.csv'"]}],"source":["import pandas as pd\n","import numpy as np\n","from collections import defaultdict\n","import dill\n","\n","# atc -> cid\n","ddi_file = 'drug-DDI.csv'\n","cid_atc = 'drug-atc.csv'\n","voc_file = 'voc_final.pkl'\n","data_path = 'records_final.pkl'\n","TOPK = 40 # topk drug-drug interaction\n","\n","records =  dill.load(open(data_path, 'rb'))\n","cid2atc_dic = defaultdict(set)\n","med_voc = dill.load(open(voc_file, 'rb'))['med_voc']\n","med_voc_size = len(med_voc.idx2word)\n","med_unique_word = [med_voc.idx2word[i] for i in range(med_voc_size)]\n","atc3_atc4_dic = defaultdict(set)\n","for item in med_unique_word:\n","    atc3_atc4_dic[item[:4]].add(item)\n","\n","\n","with open(cid_atc, 'r') as f:\n","    for line in f:\n","        line_ls = line[:-1].split(',')\n","        cid = line_ls[0]\n","        atcs = line_ls[1:]\n","        for atc in atcs:\n","            if len(atc3_atc4_dic[atc[:4]]) != 0:\n","                cid2atc_dic[cid].add(atc[:4])\n","\n","# ddi load\n","ddi_df = pd.read_csv(ddi_file)\n","# fliter sever side effect\n","ddi_most_pd = ddi_df.groupby(by=['Polypharmacy Side Effect', 'Side Effect Name']).size().reset_index().rename(columns={0:'count'}).sort_values(by=['count'],ascending=False).reset_index(drop=True)\n","ddi_most_pd = ddi_most_pd.iloc[-TOPK:,:]\n","# ddi_most_pd = pd.DataFrame(columns=['Side Effect Name'], data=['as','asd','as'])\n","fliter_ddi_df = ddi_df.merge(ddi_most_pd[['Side Effect Name']], how='inner', on=['Side Effect Name'])\n","ddi_df = fliter_ddi_df[['STITCH 1','STITCH 2']].drop_duplicates().reset_index(drop=True)\n","\n","# weighted ehr adj\n","ehr_adj = np.zeros((med_voc_size, med_voc_size))\n","for patient in records:\n","    for adm in patient:\n","        med_set = adm[2]\n","        for i, med_i in enumerate(med_set):\n","            for j, med_j in enumerate(med_set):\n","                if j<=i:\n","                    continue\n","                ehr_adj[med_i, med_j] = 1\n","                ehr_adj[med_j, med_i] = 1\n","dill.dump(ehr_adj, open('ehr_adj_final.pkl', 'wb'))\n","\n","\n","\n","# ddi adj\n","ddi_adj = np.zeros((med_voc_size,med_voc_size))\n","for index, row in ddi_df.iterrows():\n","    # ddi\n","    cid1 = row['STITCH 1']\n","    cid2 = row['STITCH 2']\n","\n","    # cid -> atc_level3\n","    for atc_i in cid2atc_dic[cid1]:\n","        for atc_j in cid2atc_dic[cid2]:\n","\n","            # atc_level3 -> atc_level4\n","            for i in atc3_atc4_dic[atc_i]:\n","                for j in atc3_atc4_dic[atc_j]:\n","                    if med_voc.word2idx[i] != med_voc.word2idx[j]:\n","                        ddi_adj[med_voc.word2idx[i], med_voc.word2idx[j]] = 1\n","                        ddi_adj[med_voc.word2idx[j], med_voc.word2idx[i]] = 1\n","dill.dump(ddi_adj, open('ddi_A_final.pkl', 'wb'))\n","\n","print('complete!')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}